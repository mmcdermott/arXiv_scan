{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8a44fed-48a8-4f83-8f3b-e258de3d13a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import copy\n",
    "import json\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "ARXIV_DIR = Path(\"/n/data1/hms/dbmi/zaklab/arXiv\")\n",
    "OPENAI_RESULTS_FP = ARXIV_DIR / \"openai_results.pkl\"\n",
    "has_terms_fp = ARXIV_DIR / \"has_terms.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0e05e1-cf6b-4b1e-986f-2ecc7bd90f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "scanned_fps_fp = ARXIV_DIR / \"scanned_txts.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5237ce7e-6d41-43a0-b9a9-d8b22dfbb77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 863282 files total.\n"
     ]
    }
   ],
   "source": [
    "n_files = len(scanned_fps_fp.read_text().split('\\n'))\n",
    "print(f\"Processed {n_files} files total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c02d0a-f6e0-48f3-b072-a49aafdca503",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_TERMS = (\n",
    "    [\"AUC\", \"AUROC\", \"Area under the receiver operating characteristic\", \"ROC\", \"receiver operating characteristic\"],\n",
    "    [\"APR\", \"AUPRC\", \"Area under the precision recall curve\", \"Average precision\", \"PRC\", \"Precision recall curve\"],\n",
    ")\n",
    "\n",
    "def query(text: str, text_queries=SEARCH_TERMS):\n",
    "    match text_queries:\n",
    "        case list() as or_queries:\n",
    "            for q in or_queries:\n",
    "                if query(text, q):\n",
    "                    return True\n",
    "            return False\n",
    "        case str() as q:\n",
    "            return re.search(r\"(?:\\W|^)\" + q + r\"(?:\\W|$)\", text, flags=re.I)\n",
    "        case tuple() as and_queries:\n",
    "            for q in and_queries:\n",
    "                if not query(text, q):\n",
    "                    return False\n",
    "            return True\n",
    "        case _:\n",
    "            raise TypeError(f\"Can only accept lists (or), tuples (and), and strings (queries). Got {type(text_queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dc038ea-6258-4a21-b9d6-469211bd28eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1686 files to check.\n"
     ]
    }
   ],
   "source": [
    "with_terms_fps = [x.strip() for x in has_terms_fp.read_text().split('\\n') if x.strip()]\n",
    "print(f\"Loaded {len(with_terms_fps)} files to check.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f12e9c2-feb5-4bfa-9ec3-d87a84dbdb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENAI_RESULTS_FP.is_file():\n",
    "    with open(OPENAI_RESULTS_FP, mode='rb') as f:\n",
    "        old_results = pickle.load(f)\n",
    "        old_with_terms_fps = old_results[\"with_terms_fps\"]\n",
    "        old_with_terms_chunk = old_results[\"with_terms_chunk\"]\n",
    "        old_with_terms_openai = old_results[\"with_terms_openai\"]\n",
    "        old_final_docs = old_results[\"final_docs\"]\n",
    "else:\n",
    "    old_with_terms_fps = []\n",
    "    old_with_terms_chunk = []\n",
    "    old_with_terms_openai = []\n",
    "    old_final_docs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45228cec-2b50-4d94-8fe4-85491bd59d03",
   "metadata": {},
   "source": [
    "### Pre-filter by chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b20f257-1f6d-4159-9461-1f2bec5ff330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_text_detailed(fp: Path, chunk_size: int, offset: int) -> list[str]:\n",
    "    doc = fp.read_text().lower()\n",
    "    as_words = doc.split() \n",
    "    \n",
    "    matching_chunks = []\n",
    "    for st in range(0, len(as_words), offset):\n",
    "        chunk = ' '.join(as_words[st:st+chunk_size])\n",
    "        if query(chunk):\n",
    "            matching_chunks.append((st, chunk))\n",
    "    \n",
    "    return matching_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f2c3d06-2730-4808-ab92-77713cacf728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86cad6b45f6e4282ad337bb43182395b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1686 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with_terms_localized = copy.deepcopy(old_with_terms_chunk)\n",
    "\n",
    "for fp in tqdm(with_terms_fps):\n",
    "    if fp in old_with_terms_fps: continue\n",
    "    chunks = check_text_detailed(Path(fp), 512, 128)\n",
    "    if chunks:\n",
    "        with_terms_localized.append((fp, chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84d8a834-65e3-4aea-8f40-f4b8d85c428e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found total of 516 documents with relevant chunks.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found total of {len(with_terms_localized)} documents with relevant chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ef5d116-afe5-4d8d-bd9c-324b82f256e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_tokens(doc: str) -> int:\n",
    "    return len(enc.encode(doc))\n",
    "\n",
    "MODELS = {\n",
    "    \"GPT-4 Turbo\": (0.01, 0.03, 128000),\n",
    "    \"GPT-3.5 Turbo\": (0.001, 0.002, 16000),\n",
    "    \"GPT-4\": (0.03, 0.06, 8192),\n",
    "}\n",
    "\n",
    "def profile_cost(fp: str | Path) -> dict[str, int]:\n",
    "    n = n_tokens((Path(fp) if type(fp) is str else fp).read_text())\n",
    "    \n",
    "    cost_dict = {k: in_c*n + 100*out_c for k, (in_c, out_c, _) in MODELS.items()}\n",
    "    return cost_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1b88c6-6dda-4480-8ea5-17cf65a4b952",
   "metadata": {},
   "source": [
    "### Query with GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "554d97d7-4254-4b26-9677-6963eee1c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are an expert in machine learning and scientific literature review. \"\n",
    "    \"For each chunk of a published paper (which may have typos, misspellings, and odd characters as a result of conversion from PDF), \"\n",
    "    \"return a JSON object that states whether or not the paper makes any claim that the area under the precision recall curve (AUPRC) \"\n",
    "    \"is superior as a general performance metric to the area under the receiver operating characteristic (AUROC) in an ML setting. \"\n",
    "    \"A paper claiming that a model performs better under AUPRC vs. AUROC is *not* an example of this; instead a paper claiming that AUPRC \"\n",
    "    \"should be used instead of AUROC in cases of class imbalance is an example of this metric commentary. \"\n",
    "    \"Respond with format {'claims': [{'claim': DESCRIPTION OF CLAIM, 'evidence': SUBSTRING FROM INPUT STATING CLAIM}, ...]}. \"\n",
    "    \"If the paper makes no claims, leave the 'claims' key in the JSON object empty.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48a3fc85-650c-4a8d-9c69-be907dafeb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_with_openai(\n",
    "    chunks: list[tuple[int, str]], model: str, system_prompt: str\n",
    "):\n",
    "    client = OpenAI()\n",
    "\n",
    "    responses = []\n",
    "    for st_idx, chunk in chunks:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                response_format={ \"type\": \"json_object\" },\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": chunk},\n",
    "                ]\n",
    "            )\n",
    "            as_json = json.loads(response.choices[0].message.content)\n",
    "            if \"claims\" in as_json: responses.extend(as_json[\"claims\"])\n",
    "        except:\n",
    "            print(\"Failed!\")\n",
    "            continue\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee311562-d03e-4bd3-a2dc-f71b43c4af2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7d7977dad74d0592875ab747c0090a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1609/1609.04392v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1609/1609.03536v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1609/1609.09430v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1609/1609.04392v6.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1609/1609.04392v4.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1603/1603.09114v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1504/1504.03106v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1012/1012.0930v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1110/1110.4198v3.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1110/1110.4198v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1205/1205.6986v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1703/1703.04213v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1703/1703.04213v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1703/1703.00564v3.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1703/1703.04559v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1507/1507.03228v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1310/1310.7501v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1612/1612.01848v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1612/1612.06053v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1612/1612.07736v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1701/1701.00995v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1701/1701.00995v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1602/1602.01160v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1206/1206.4667v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1304/1304.6245v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1312/1312.7570v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1505/1505.06813v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1202/1202.3701v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1606/1606.05413v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1606/1606.04637v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1704/1704.06176v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1704/1704.06176v5.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1704/1704.06176v3.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1704/1704.06176v4.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1508/1508.02593v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1508/1508.02803v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1508/1508.04123v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1402/1402.7322v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1402/1402.1216v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1407/1407.1151v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1611/1611.09159v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1611/1611.08563v5.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1605/1605.06359v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1610/1610.04662v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1610/1610.04662v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1610/1610.00192v3.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1702/1702.02215v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1702/1702.00615v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1702/1702.06877v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1702/1702.06877v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1307/1307.8046v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1001/1001.4019v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1303/1303.6385v3.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1303/1303.4015v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1303/1303.2054v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1406/1406.7250v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1512/1512.06900v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1512/1512.07851v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1512/1512.07851v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1512/1512.01616v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1512/1512.00743v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1512/1512.01088v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1107/1107.2462v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1408/1408.5634v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1306/1306.6709v4.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1401/1401.4590v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1405/1405.6524v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1412/1412.8556v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1510/1510.04445v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1607/1607.07607v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1607/1607.00872v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1607/1607.08329v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1607/1607.04853v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1607/1607.04853v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1212/1212.4569v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1604/1604.05210v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1604/1604.04960v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1604/1604.08269v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1604/1604.07090v5.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1604/1604.06506v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1604/1604.07090v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1506/1506.00511v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1506/1506.02816v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1506/1506.02816v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1608/1608.04802v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1404/1404.5331v2.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1404/1404.5943v1.txt\n",
      "/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1211/1211.1275v3.txt\n"
     ]
    }
   ],
   "source": [
    "with_terms_openai = copy.deepcopy(old_with_terms_openai)\n",
    "for fp, chunks in tqdm(with_terms_localized):\n",
    "    if fp in old_with_terms_fps:\n",
    "        continue\n",
    "    openai_chunks = check_with_openai(chunks, model=\"gpt-3.5-turbo-1106\", system_prompt=SYSTEM_PROMPT)\n",
    "    if openai_chunks:\n",
    "        print(fp)\n",
    "        with_terms_openai.append((fp, chunks, openai_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e9d262d-dfac-455e-ae18-e9367a8121ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(with_terms_openai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d16393-7e8a-42c0-a610-a8b93973fb80",
   "metadata": {},
   "source": [
    "### Validate with GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be7f7951-ce70-4a1b-a4d9-38fafe8796bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_doc_with_openai(\n",
    "    doc: str, model: str, system_prompt: str\n",
    "):\n",
    "    client = OpenAI()\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": doc},\n",
    "            ]\n",
    "        )\n",
    "        as_json = json.loads(response.choices[0].message.content)\n",
    "        if \"claims\" in as_json: return as_json[\"claims\"]\n",
    "        else: return []\n",
    "    except Exception as e:\n",
    "        print(f\"Failed with {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6923dd86-993d-48ff-a352-ae78f2bf691e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516b0c71ed2841f5abd61f8618f81e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 relevant final documents.\n"
     ]
    }
   ],
   "source": [
    "final_docs = copy.deepcopy(old_final_docs)\n",
    "for fp, chunks, openai_chunks in tqdm(with_terms_openai):\n",
    "    if fp in old_with_terms_fps:\n",
    "        continue\n",
    "    final_doc_response_claims = check_doc_with_openai(Path(fp).read_text(), model=\"gpt-4-1106-preview\", system_prompt=SYSTEM_PROMPT)\n",
    "    if final_doc_response_claims:\n",
    "        final_docs.append((fp, final_doc_response_claims))\n",
    "\n",
    "print(f\"Found {len(final_docs)} relevant final documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b65c4175-899f-4953-911b-e71b203d2b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OPENAI_RESULTS_FP, mode='wb') as f:\n",
    "    pickle.dump({\n",
    "        \"with_terms_fps\": with_terms_fps,\n",
    "        \"with_terms_chunk\": with_terms_localized,\n",
    "        \"with_terms_openai\": with_terms_openai,\n",
    "        \"final_docs\": final_docs,\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c26130c8-4cbf-4c28-940b-57be952fefb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1310/1310.5103v1.txt',\n",
       "  [{'claim': 'AP places more emphasis on the initial part of the ROC curve and addresses criticisms of the AUC',\n",
       "    'evidence': 'for the AUC, stamina and momentum are equally important, whereas for the AP, momentum is more important'}]),\n",
       " ('/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1206/1206.4667v1.txt',\n",
       "  [{'claim': 'AUPRC is preferred to AUROC in situations of large class skew',\n",
       "    'evidence': 'In particular, PR analysis is preferred to ROC analysis when there is a large skew in the class distribution.'}])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
