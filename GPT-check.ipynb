{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8a44fed-48a8-4f83-8f3b-e258de3d13a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import copy\n",
    "import json\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "ARXIV_DIR = Path(\"/n/data1/hms/dbmi/zaklab/arXiv\")\n",
    "OPENAI_RESULTS_FP = ARXIV_DIR / \"openai_results.pkl\"\n",
    "has_terms_fp = ARXIV_DIR / \"has_terms.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd0e05e1-cf6b-4b1e-986f-2ecc7bd90f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "scanned_fps_fp = ARXIV_DIR / \"scanned_txts.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5237ce7e-6d41-43a0-b9a9-d8b22dfbb77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1370141 files total.\n"
     ]
    }
   ],
   "source": [
    "n_files = len(scanned_fps_fp.read_text().split('\\n'))\n",
    "print(f\"Processed {n_files} files total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6c02d0a-f6e0-48f3-b072-a49aafdca503",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_TERMS = (\n",
    "    [\"AUC\", \"AUROC\", \"Area under the receiver operating characteristic\", \"ROC\", \"receiver operating characteristic\"],\n",
    "    [\"APR\", \"AUPRC\", \"Area under the precision recall curve\", \"Average precision\", \"PRC\", \"Precision recall curve\"],\n",
    ")\n",
    "\n",
    "def query(text: str, text_queries=SEARCH_TERMS):\n",
    "    match text_queries:\n",
    "        case list() as or_queries:\n",
    "            for q in or_queries:\n",
    "                if query(text, q):\n",
    "                    return True\n",
    "            return False\n",
    "        case str() as q:\n",
    "            return re.search(r\"(?:\\W|^)\" + q + r\"(?:\\W|$)\", text, flags=re.I)\n",
    "        case tuple() as and_queries:\n",
    "            for q in and_queries:\n",
    "                if not query(text, q):\n",
    "                    return False\n",
    "            return True\n",
    "        case _:\n",
    "            raise TypeError(f\"Can only accept lists (or), tuples (and), and strings (queries). Got {type(text_queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dc038ea-6258-4a21-b9d6-469211bd28eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4324 files to check.\n"
     ]
    }
   ],
   "source": [
    "with_terms_fps = [x.strip() for x in has_terms_fp.read_text().split('\\n') if x.strip()]\n",
    "print(f\"Loaded {len(with_terms_fps)} files to check.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f12e9c2-feb5-4bfa-9ec3-d87a84dbdb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENAI_RESULTS_FP.is_file():\n",
    "    with open(OPENAI_RESULTS_FP, mode='rb') as f:\n",
    "        old_results = pickle.load(f)\n",
    "        old_with_terms_fps = old_results[\"with_terms_fps\"]\n",
    "        old_with_terms_chunk = old_results[\"with_terms_chunk\"]\n",
    "        old_with_terms_openai = old_results[\"with_terms_openai\"]\n",
    "        old_final_docs = old_results[\"final_docs\"]\n",
    "else:\n",
    "    old_with_terms_fps = []\n",
    "    old_with_terms_chunk = []\n",
    "    old_with_terms_openai = []\n",
    "    old_final_docs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45228cec-2b50-4d94-8fe4-85491bd59d03",
   "metadata": {},
   "source": [
    "### Pre-filter by chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b20f257-1f6d-4159-9461-1f2bec5ff330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_text_detailed(fp: Path, chunk_size: int, offset: int) -> list[str]:\n",
    "    doc = fp.read_text().lower()\n",
    "    as_words = doc.split() \n",
    "    \n",
    "    matching_chunks = []\n",
    "    for st in range(0, len(as_words), offset):\n",
    "        chunk = ' '.join(as_words[st:st+chunk_size])\n",
    "        if query(chunk):\n",
    "            matching_chunks.append((st, chunk))\n",
    "    \n",
    "    return matching_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f2c3d06-2730-4808-ab92-77713cacf728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc9355cebb843f18d9146927bdc640b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with_terms_localized = copy.deepcopy(old_with_terms_chunk)\n",
    "\n",
    "for fp in tqdm(with_terms_fps):\n",
    "    if fp in old_with_terms_fps: continue\n",
    "    chunks = check_text_detailed(Path(fp), 512, 128)\n",
    "    if chunks:\n",
    "        with_terms_localized.append((fp, chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84d8a834-65e3-4aea-8f40-f4b8d85c428e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found total of 836 documents with relevant chunks.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found total of {len(with_terms_localized)} documents with relevant chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ef5d116-afe5-4d8d-bd9c-324b82f256e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_tokens(doc: str) -> int:\n",
    "    return len(enc.encode(doc))\n",
    "\n",
    "MODELS = {\n",
    "    \"GPT-4 Turbo\": (0.01, 0.03, 128000),\n",
    "    \"GPT-3.5 Turbo\": (0.001, 0.002, 16000),\n",
    "    \"GPT-4\": (0.03, 0.06, 8192),\n",
    "}\n",
    "\n",
    "def profile_cost(fp: str | Path) -> dict[str, int]:\n",
    "    n = n_tokens((Path(fp) if type(fp) is str else fp).read_text())\n",
    "    \n",
    "    cost_dict = {k: in_c*n + 100*out_c for k, (in_c, out_c, _) in MODELS.items()}\n",
    "    return cost_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1b88c6-6dda-4480-8ea5-17cf65a4b952",
   "metadata": {},
   "source": [
    "### Query with GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "554d97d7-4254-4b26-9677-6963eee1c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are an expert in machine learning and scientific literature review. \"\n",
    "    \"For each chunk of a published paper (which may have typos, misspellings, and odd characters as a result of conversion from PDF), \"\n",
    "    \"return a JSON object that states whether or not the paper makes any claim that the area under the precision recall curve (AUPRC) \"\n",
    "    \"is superior as a general performance metric to the area under the receiver operating characteristic (AUROC) in an ML setting. \"\n",
    "    \"A paper claiming that a model performs better under AUPRC vs. AUROC is *not* an example of this; instead a paper claiming that AUPRC \"\n",
    "    \"should be used instead of AUROC in cases of class imbalance is an example of this metric commentary. \"\n",
    "    \"Respond with format {'claims': [{'claim': DESCRIPTION OF CLAIM, 'evidence': SUBSTRING FROM INPUT STATING CLAIM}, ...]}. \"\n",
    "    \"If the paper makes no claims, leave the 'claims' key in the JSON object empty.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48a3fc85-650c-4a8d-9c69-be907dafeb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_with_openai(\n",
    "    chunks: list[tuple[int, str]], model: str, system_prompt: str\n",
    "):\n",
    "    client = OpenAI()\n",
    "\n",
    "    responses = []\n",
    "    for st_idx, chunk in chunks:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                response_format={ \"type\": \"json_object\" },\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": chunk},\n",
    "                ]\n",
    "            )\n",
    "            as_json = json.loads(response.choices[0].message.content)\n",
    "            if \"claims\" in as_json: responses.extend(as_json[\"claims\"])\n",
    "        except:\n",
    "            print(\"Failed!\")\n",
    "            continue\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee311562-d03e-4bd3-a2dc-f71b43c4af2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38055510c7d8418984ed00a978830565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/836 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed!\n",
      "Failed!\n",
      "Failed!\n"
     ]
    }
   ],
   "source": [
    "with_terms_openai = copy.deepcopy(old_with_terms_openai)\n",
    "for fp, chunks in tqdm(with_terms_localized):\n",
    "    if fp in old_with_terms_fps:\n",
    "        continue\n",
    "    openai_chunks = check_with_openai(chunks, model=\"gpt-3.5-turbo-1106\", system_prompt=SYSTEM_PROMPT)\n",
    "    if openai_chunks:\n",
    "        with_terms_openai.append((fp, chunks, openai_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e9d262d-dfac-455e-ae18-e9367a8121ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 248 that GPT 3.5 thinks are relevant\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(with_terms_openai)} that GPT 3.5 thinks are relevant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d16393-7e8a-42c0-a610-a8b93973fb80",
   "metadata": {},
   "source": [
    "### Validate with GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be7f7951-ce70-4a1b-a4d9-38fafe8796bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_doc_with_openai(\n",
    "    doc: str, model: str, system_prompt: str\n",
    "):\n",
    "    client = OpenAI()\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": doc},\n",
    "            ]\n",
    "        )\n",
    "        as_json = json.loads(response.choices[0].message.content)\n",
    "        if \"claims\" in as_json: return as_json[\"claims\"]\n",
    "        else: return []\n",
    "    except Exception as e:\n",
    "        print(f\"Failed with {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6923dd86-993d-48ff-a352-ae78f2bf691e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5e8c166a854112a9e2292e4ab11020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 relevant final documents.\n"
     ]
    }
   ],
   "source": [
    "final_docs = copy.deepcopy(old_final_docs)\n",
    "for fp, chunks, openai_chunks in tqdm(with_terms_openai):\n",
    "    if fp in old_with_terms_fps:\n",
    "        continue\n",
    "    final_doc_response_claims = check_doc_with_openai(Path(fp).read_text(), model=\"gpt-4-1106-preview\", system_prompt=SYSTEM_PROMPT)\n",
    "    if final_doc_response_claims:\n",
    "        final_docs.append((fp, final_doc_response_claims))\n",
    "\n",
    "print(f\"Found {len(final_docs)} relevant final documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b65c4175-899f-4953-911b-e71b203d2b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OPENAI_RESULTS_FP, mode='wb') as f:\n",
    "    pickle.dump({\n",
    "        \"with_terms_fps\": with_terms_fps,\n",
    "        \"with_terms_chunk\": with_terms_localized,\n",
    "        \"with_terms_openai\": with_terms_openai,\n",
    "        \"final_docs\": final_docs,\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c26130c8-4cbf-4c28-940b-57be952fefb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1310/1310.5103v1.txt',\n",
       "  [{'claim': 'AP places more emphasis on the initial part of the ROC curve and addresses criticisms of the AUC',\n",
       "    'evidence': 'for the AUC, stamina and momentum are equally important, whereas for the AP, momentum is more important'}]),\n",
       " ('/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1206/1206.4667v1.txt',\n",
       "  [{'claim': 'AUPRC is preferred to AUROC in situations of large class skew',\n",
       "    'evidence': 'In particular, PR analysis is preferred to ROC analysis when there is a large skew in the class distribution.'}]),\n",
       " ('/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1206/1206.4667v2.txt',\n",
       "  [{'claim': 'AUPRC is preferred to AUROC when there is large class distribution skew',\n",
       "    'evidence': 'In particular, PR analysis is preferred to ROC analysis when there is a large skew in the class distribution.'},\n",
       "   {'claim': 'Unachievable region in PR space leads to minimum AUCPR, influencing evaluation',\n",
       "    'evidence': 'The unachievable points define a minimum PR curve. The area under the minimum PR curve constitutes a portion of AUCPR that any algorithm, no matter how poor, is guaranteed to obtain “for free.”'},\n",
       "   {'claim': 'Normalized AUCPR (AUCNPR) accounts for the unachievable region and is proposed as a more appropriate measure',\n",
       "    'evidence': 'These undesirable effects of the unachievable region can be at least partially offset with straightforward modifications to AUCPR, which we describe.'}]),\n",
       " ('/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1511/1511.02196v1.txt',\n",
       "  [{'claim': 'The AUPRC is more relevant than the AUROC in contexts where high precision is crucial, such as when a model predicts positive interactions with very high confidence to save efforts in laboratory',\n",
       "    'evidence': 'we start from the goal of machine learning models applied to accelerate laboratory biology research, propose an evaluation metric that can describe the ability of a predictor to be adopted by biology researchers. We stress on that a machine learning model should predict positive interactions with very high confidence, thus that efforts are saved in laboratory.'}]),\n",
       " ('/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1403/1403.7100v1.txt',\n",
       "  [{'claim': \"The paper claims that measures such as BER and AUCb are 'proper' for processing class-imbalanced problems, while measures like AT, F1, and GPR are 'improper' according to their cost functions analysis.\",\n",
       "    'evidence': \"This type of cost functions shows a 'proper' feature in processing class-imbalanced problems, because it satisfies the meta measure... The 'proper' kind includes the four means on accuracy rates and BER(equivalently including AUCb). The other measures, i.e. AT, the four means on precision and recall (including F1), MCC and κ, belong to 'improper' kind.\"}]),\n",
       " ('/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1303/1303.4015v1.txt',\n",
       "  [{'claim': 'AUPRC is implicitly advocated over AUROC when dealing with imbalanced classes, as minimizing the norm of the confusion matrix helps to smooth the accuracy among imbalanced classes, giving minority classes as much importance as majority classes.',\n",
       "    'evidence': 'In this work, we advocate that minimizing the norm of the confusion matrix is helpful for smoothing the accuracy among imbalanced classes, so that minority classes are considered as important as majority classes.'}]),\n",
       " ('/n/data1/hms/dbmi/zaklab/arXiv/arxiv_as_txt/arxiv/pdf/1606/1606.04172v1.txt',\n",
       "  [{'claim': 'AUPRC is potentially more informative and appropriate than AUROC for class imbalance in event prediction for censored time-to-event data.',\n",
       "    'evidence': 'A number of authors pointed out that the AUC metric is informative on the classification performance and discrimination power [...], but not necessarily an appropriate metric for assessing the prospective accuracy performance (Moskowitz and Pepe 2004; Zheng et al. 2008). The proposed AP provides an alternative accuracy measure helpful for making informed clinical decisions.'}])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000cda95-f4df-4c28-8bf3-31030837a51b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
